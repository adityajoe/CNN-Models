{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Official Documentation: https://pytorch.org/hub/pytorch_vision_alexnet/"
      ],
      "metadata": {
        "id": "qofqhckSIkOS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUEIgfV7NK4R",
        "outputId": "b0dd9611-6a9b-49db-b4c6-35d49db08467"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary\n",
        "from google.colab import drive\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "drive.mount('/content/drive')\n",
        "dataset_path = \"/content/drive/MyDrive/Trade_finance_small_dataset\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0XuRAgnjflt",
        "outputId": "0c7e166e-81d9-471f-9abe-dc02d09ef9ab"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "num_classes = 5\n",
        "model.classifier[6] = torch.nn.Linear(4096, num_classes)"
      ],
      "metadata": {
        "id": "WauXZK40ystX"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary(model, (3, 224, 224))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTZtzJ11j9qJ",
        "outputId": "fad80e93-0d74-4e08-c089-caae9c5cc954"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 55, 55]          23,296\n",
            "              ReLU-2           [-1, 64, 55, 55]               0\n",
            "         MaxPool2d-3           [-1, 64, 27, 27]               0\n",
            "            Conv2d-4          [-1, 192, 27, 27]         307,392\n",
            "              ReLU-5          [-1, 192, 27, 27]               0\n",
            "         MaxPool2d-6          [-1, 192, 13, 13]               0\n",
            "            Conv2d-7          [-1, 384, 13, 13]         663,936\n",
            "              ReLU-8          [-1, 384, 13, 13]               0\n",
            "            Conv2d-9          [-1, 256, 13, 13]         884,992\n",
            "             ReLU-10          [-1, 256, 13, 13]               0\n",
            "           Conv2d-11          [-1, 256, 13, 13]         590,080\n",
            "             ReLU-12          [-1, 256, 13, 13]               0\n",
            "        MaxPool2d-13            [-1, 256, 6, 6]               0\n",
            "AdaptiveAvgPool2d-14            [-1, 256, 6, 6]               0\n",
            "          Dropout-15                 [-1, 9216]               0\n",
            "           Linear-16                 [-1, 4096]      37,752,832\n",
            "             ReLU-17                 [-1, 4096]               0\n",
            "          Dropout-18                 [-1, 4096]               0\n",
            "           Linear-19                 [-1, 4096]      16,781,312\n",
            "             ReLU-20                 [-1, 4096]               0\n",
            "           Linear-21                    [-1, 5]          20,485\n",
            "================================================================\n",
            "Total params: 57,024,325\n",
            "Trainable params: 20,485\n",
            "Non-trainable params: 57,003,840\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 8.37\n",
            "Params size (MB): 217.53\n",
            "Estimated Total Size (MB): 226.48\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = os.listdir(dataset_path)\n",
        "classes\n",
        "label_dict = {value: idx for idx, value in enumerate(classes)}\n",
        "id_label_dict = {idx: value for idx, value in enumerate(classes)}"
      ],
      "metadata": {
        "id": "keUucXPDOMG-"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id_label_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1gABsuB5JHT",
        "outputId": "7d40b541-51ed-475f-a55b-adef86fc8336"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'packing_list',\n",
              " 1: 'bill_of_lading_first_page',\n",
              " 2: 'certificate_of_origin_first_page',\n",
              " 3: 'Insurance_Certificate_pngs_first_page',\n",
              " 4: 'covering_schedule'}"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images_dict = {\n",
        "            'images': [],  # List of train images as PIL images\n",
        "            'labels': []  # List of corresponding labels (strings) for train images\n",
        "        }\n",
        "\n",
        "test_images_dict = {\n",
        "            'images': [],\n",
        "            'labels': []\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "EfnU1QUnTDxO"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYQjO4_ewL3m",
        "outputId": "7b219859-4a30-489d-db91-ea97c13c2dbb"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mAlexnetModels\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/AlexNet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XE5DDvCkutdK",
        "outputId": "4338afc1-517b-4b5f-a468-6fd8338ca700"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AlexNet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir(\"train_images\")\n",
        "os.mkdir(\"test_images\")"
      ],
      "metadata": {
        "id": "iCXadwChw-E1"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import shutil\n",
        "from PIL import Image\n",
        "random.seed(10)\n",
        "train_files = []\n",
        "test_files = []\n",
        "for document in os.listdir(dataset_path):\n",
        "  document_path = os.path.join(dataset_path,document)\n",
        "  files= os.listdir(os.path.join(dataset_path,document))\n",
        "  random.shuffle(files)\n",
        "  for train_file in files[:-int(0.2 * len(files))]:\n",
        "    if train_file not in os.listdir(\"train_images\"):\n",
        "      shutil.copy(os.path.join(document_path, train_file), \"train_images\")\n",
        "      img = Image.open(os.path.join(document_path, train_file)).convert(\"RGB\")\n",
        "      train_images_dict[\"images\"].append(img)\n",
        "      train_images_dict[\"labels\"].append(label_dict[document])\n",
        "  for test_file in files[-int(0.2 * len(files)):]:\n",
        "    if test_file not in os.listdir(\"test_images\"):\n",
        "      shutil.copy(os.path.join(document_path, test_file), \"test_images\")\n",
        "      img = Image.open(os.path.join(document_path, test_file)).convert(\"RGB\")\n",
        "      test_images_dict[\"images\"].append(img)\n",
        "      test_images_dict[\"labels\"].append(label_dict[document])\n"
      ],
      "metadata": {
        "id": "F4Gphw9vZ2ql"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "    '''\n",
        "        Sample Input:\n",
        "        train_images = {\n",
        "            'images': [PIL_image1, PIL_image2, PIL_image3, ...],  # List of train images as PIL images\n",
        "            'labels': ['cat', 'dog', 'bird', ...]  # List of corresponding labels (strings) for train images\n",
        "        }\n",
        "\n",
        "        test_images = {\n",
        "            'images': [PIL_image4, PIL_image5, PIL_image6, ...],\n",
        "            'labels': ['cat', 'dog', 'bird', ...]\n",
        "        }\n",
        "\n",
        "        val_images = {\n",
        "            'images': [PIL_image7, PIL_image8, PIL_image9, ...],\n",
        "            'labels': ['cat', 'dog', 'bird', ...]\n",
        "        }\n",
        "    '''\n",
        "    def __init__(self, image_dict, transform=None):\n",
        "        self.image_dict = image_dict\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_dict['images'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.image_dict['images'][idx]\n",
        "        label = self.image_dict['labels'][idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Convert label to tensor\n",
        "        label = torch.tensor(int(label))\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "zGNZC0lEXoqF"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mean and standard deviation are calculated on the training set of the ImageNet dataset, which consists of a large number of RGB images. By normalizing the input image using the same mean and standard deviation values, you are aligning the input data distribution with the data distribution seen during training.\n",
        "\n",
        "The mean values represent the **average intensity of the red, green, and blue channels across the ImageNet training set**. The standard deviation values indicate the variation or spread of the pixel values within each channel.\n",
        "\n",
        "When you normalize an input image using these mean and standard deviation values, each channel's pixel values will be adjusted to have a mean of approximately 0 and a standard deviation of approximately 1. This normalization step helps the model to handle the input data more effectively and can improve training convergence and performance.\n",
        "\n",
        "It's important to note that these mean and standard deviation values are specific to the ImageNet dataset and models trained on it. If you're working with a different dataset or a model trained on a different dataset, you may need to use different mean and standard deviation values for normalization based on the statistics of your specific dataset.\n",
        "\n",
        "```python\n",
        "input_normalized = (input_tensor - mean) / std\n",
        "```\n",
        "- `input_tensor` values range [0,1]\n",
        "- `input_tensor` shape -> (Channels, Height, Width) ; (3, 224, 224) in AlexNet"
      ],
      "metadata": {
        "id": "oePf7BVoX_vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageTransformer:\n",
        "    def __init__(self):\n",
        "        self._img_size = 256\n",
        "        self._img_crop_size = 224\n",
        "        self._mean = [0.485, 0.456, 0.406]\n",
        "        self._std = [0.229, 0.224, 0.225]\n",
        "        self._required_channels = \"RGB\"\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(self._img_size),\n",
        "            transforms.CenterCrop(self._img_crop_size),\n",
        "            transforms.Lambda(lambda x: x.convert(self._required_channels) if x.mode != self._required_channels else x),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=self._mean, std=self._std)\n",
        "        ])"
      ],
      "metadata": {
        "id": "4eLP9vU94F8H"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Data"
      ],
      "metadata": {
        "id": "fq9QAb70cDjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the data loaders for train, test, and validation\n",
        "train_dataset = ImageDataset(train_images_dict, transform=ImageTransformer().transform)\n",
        "test_dataset = ImageDataset(test_images_dict, transform=ImageTransformer().transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "vFRA1CMDbwrX"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "IhBX-bFDkjyJ"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(epoch_index):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "    for i, data in enumerate(train_loader):\n",
        "      #print(data)\n",
        "      # Every data instance is an input + label pair\n",
        "      inputs, labels = data\n",
        "      print(\"input is \", inputs[0].shape)\n",
        "\n",
        "      # Zero your gradients for every batch!\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Make predictions for this batch\n",
        "      output = model(inputs)\n",
        "      output[0].shape\n",
        "      # Compute the loss and its gradients\n",
        "      loss = loss_fn(output, labels)\n",
        "      loss.backward()\n",
        "\n",
        "      # Adjust learning weights\n",
        "      optimizer.step()\n",
        "\n",
        "      # Gather data and report\n",
        "      running_loss += loss.item()\n",
        "    last_loss = running_loss /10    # loss per batch\n",
        "\n",
        "    return last_loss"
      ],
      "metadata": {
        "id": "BwEIgDwjePfD"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
        "from datetime import datetime\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "#writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n"
      ],
      "metadata": {
        "id": "oZjJ_548qVAX"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "WNztJkYRiCuu",
        "outputId": "2fb0152e-c66f-4d69-9cac-7fefda367bd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1:\n",
            "input is  torch.Size([3, 224, 224])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "LOSS train 0.4718326926231384 test 1.2501769065856934\n",
            "EPOCH 2:\n",
            "input is  torch.Size([3, 224, 224])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "LOSS train 0.3501650333404541 test 0.9933612942695618\n",
            "EPOCH 3:\n",
            "input is  torch.Size([3, 224, 224])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "LOSS train 0.27775235176086427 test 0.9115647077560425\n",
            "EPOCH 4:\n",
            "input is  torch.Size([3, 224, 224])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "LOSS train 0.2501747727394104 test 0.8654217720031738\n",
            "EPOCH 5:\n",
            "input is  torch.Size([3, 224, 224])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "LOSS train 0.23672982454299926 test 0.8386901021003723\n",
            "EPOCH 6:\n",
            "input is  torch.Size([3, 224, 224])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "LOSS train 0.17984691262245178 test 0.7462583780288696\n",
            "EPOCH 7:\n",
            "input is  torch.Size([3, 224, 224])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "LOSS train 0.1903354287147522 test 0.7074246406555176\n",
            "EPOCH 8:\n",
            "input is  torch.Size([3, 224, 224])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "LOSS train 0.14378171265125275 test 0.6962630748748779\n",
            "EPOCH 9:\n",
            "input is  torch.Size([3, 224, 224])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "LOSS train 0.13379227221012116 test 0.6863933205604553\n",
            "EPOCH 10:\n",
            "input is  torch.Size([3, 224, 224])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "LOSS train 0.12226688265800476 test 0.6553316116333008\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "epoch_number = 0\n",
        "best_vloss = 1_000_000.\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    model.train(True)\n",
        "    avg_loss = train_one_epoch(epoch_number)\n",
        "\n",
        "    # We don't need gradients on to do reporting\n",
        "    model.train(False)\n",
        "\n",
        "    running_test_loss = 0.0\n",
        "    for i, test_data in enumerate(test_loader):\n",
        "        test_inputs, test_labels = test_data\n",
        "        test_outputs = model(test_inputs)\n",
        "        test_loss = loss_fn(test_outputs, test_labels)\n",
        "        running_test_loss += test_loss\n",
        "\n",
        "    avg_test_loss = running_test_loss / (i + 1)\n",
        "    print('LOSS train {} test {}'.format(avg_loss, avg_test_loss))\n",
        "\n",
        "    # Log the running loss averaged per batch\n",
        "    # for both training and validation\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    # if avg_test_loss < best_vloss:\n",
        "    #     best_vloss = avg_test_loss\n",
        "    #     model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
        "    #     torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    epoch_number += 1\n",
        "    if epoch_number == 5 or epoch_number == 10:\n",
        "      path = \"AlexnetModels/model_new_{}epochs\".format(epoch_number)\n",
        "      torch.save(model, path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "czwPh-Ld5_0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Bdv2y8bHT9s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}