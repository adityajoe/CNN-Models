{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUEIgfV7NK4R",
        "outputId": "363c8487-cc89-4152-adfd-e07dac928b19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary\n",
        "from google.colab import drive\n",
        "import os\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "drive.mount('/content/drive')\n",
        "dataset_path = \"/content/drive/MyDrive/Trade_finance_small_dataset\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.efficientnet_v2_s(progress = True, weights = \"DEFAULT\")"
      ],
      "metadata": {
        "id": "7zNKZNudNL4Z"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 5\n",
        "model.classifier = nn.Linear(1280, num_classes)"
      ],
      "metadata": {
        "id": "WauXZK40ystX"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = os.listdir(dataset_path)\n",
        "classes\n",
        "label_dict = {value: idx for idx, value in enumerate(classes)}\n",
        "id_label_dict = {idx: value for idx, value in enumerate(classes)}"
      ],
      "metadata": {
        "id": "keUucXPDOMG-"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id_label_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1gABsuB5JHT",
        "outputId": "26c90ad8-9572-4a0b-c239-de87e8ca8349"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'packing_list',\n",
              " 1: 'bill_of_lading_first_page',\n",
              " 2: 'certificate_of_origin_first_page',\n",
              " 3: 'Insurance_Certificate_pngs_first_page',\n",
              " 4: 'covering_schedule'}"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images_dict = {\n",
        "            'images': [],  # List of train images as PIL images\n",
        "            'labels': []  # List of corresponding labels (strings) for train images\n",
        "        }\n",
        "\n",
        "test_images_dict = {\n",
        "            'images': [],\n",
        "            'labels': []\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "EfnU1QUnTDxO"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XE5DDvCkutdK",
        "outputId": "4cf79491-6135-4078-dc49-cd9969295c66"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'drive/MyDrive'\n",
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir(\"train_images\")\n",
        "os.mkdir(\"test_images\")"
      ],
      "metadata": {
        "id": "iCXadwChw-E1"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import shutil\n",
        "from PIL import Image\n",
        "random.seed(10)\n",
        "train_files = []\n",
        "test_files = []\n",
        "for document in os.listdir(dataset_path):\n",
        "  document_path = os.path.join(dataset_path,document)\n",
        "  files= os.listdir(os.path.join(dataset_path,document))\n",
        "  random.shuffle(files)\n",
        "  for train_file in files[:-int(0.2 * len(files))]:\n",
        "    if train_file not in os.listdir(\"train_images\"):\n",
        "      shutil.copy(os.path.join(document_path, train_file), \"train_images\")\n",
        "      img = Image.open(os.path.join(document_path, train_file)).convert(\"RGB\")\n",
        "      train_images_dict[\"images\"].append(img)\n",
        "      train_images_dict[\"labels\"].append(label_dict[document])\n",
        "  for test_file in files[-int(0.2 * len(files)):]:\n",
        "    if test_file not in os.listdir(\"test_images\"):\n",
        "      shutil.copy(os.path.join(document_path, test_file), \"test_images\")\n",
        "      img = Image.open(os.path.join(document_path, test_file)).convert(\"RGB\")\n",
        "      test_images_dict[\"images\"].append(img)\n",
        "      test_images_dict[\"labels\"].append(label_dict[document])\n"
      ],
      "metadata": {
        "id": "F4Gphw9vZ2ql"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "    '''\n",
        "        Sample Input:\n",
        "        train_images = {\n",
        "            'images': [PIL_image1, PIL_image2, PIL_image3, ...],  # List of train images as PIL images\n",
        "            'labels': ['cat', 'dog', 'bird', ...]  # List of corresponding labels (strings) for train images\n",
        "        }\n",
        "\n",
        "        test_images = {\n",
        "            'images': [PIL_image4, PIL_image5, PIL_image6, ...],\n",
        "            'labels': ['cat', 'dog', 'bird', ...]\n",
        "        }\n",
        "\n",
        "        val_images = {\n",
        "            'images': [PIL_image7, PIL_image8, PIL_image9, ...],\n",
        "            'labels': ['cat', 'dog', 'bird', ...]\n",
        "        }\n",
        "    '''\n",
        "    def __init__(self, image_dict, transform=None):\n",
        "        self.image_dict = image_dict\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_dict['images'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.image_dict['images'][idx]\n",
        "        label = self.image_dict['labels'][idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Convert label to tensor\n",
        "        label = torch.tensor(int(label))\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "zGNZC0lEXoqF"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label = 1\n",
        "a = torch.tensor(int(label))\n"
      ],
      "metadata": {
        "id": "KCrl1qCUwVnY"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageTransformer:\n",
        "    def __init__(self):\n",
        "        self._image_size = (224, 224)\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(self._image_size),\n",
        "            transforms.ToTensor(),\n",
        "            # transforms.Normalize(mean=self._mean, std=self._std)\n",
        "        ])"
      ],
      "metadata": {
        "id": "9WqCthc2YDCK"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the data loaders for train, test, and validation\n",
        "train_dataset = ImageDataset(train_images_dict, transform=ImageTransformer().transform)\n",
        "test_dataset = ImageDataset(test_images_dict, transform=ImageTransformer().transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n"
      ],
      "metadata": {
        "id": "FY8bOJMWdf8V"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "MUTjm_zrvWBd"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "2lgUfIGziCur"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(epoch_index):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "    for i, data in enumerate(train_loader):\n",
        "      #print(data)\n",
        "      # Every data instance is an input + label pair\n",
        "      inputs, labels = data\n",
        "      print(\"input is \", inputs[0].shape)\n",
        "\n",
        "      # Zero your gradients for every batch!\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Make predictions for this batch\n",
        "      outputs = model(inputs)\n",
        "      print(outputs[0].shape)\n",
        "\n",
        "      # Compute the loss and its gradients\n",
        "      loss = loss_fn(outputs, labels)\n",
        "      loss.backward()\n",
        "\n",
        "      # Adjust learning weights\n",
        "      optimizer.step()\n",
        "\n",
        "      # Gather data and report\n",
        "      running_loss += loss.item()\n",
        "    last_loss = running_loss /10    # loss per batch\n",
        "\n",
        "    return last_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
        "from datetime import datetime\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "#writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n"
      ],
      "metadata": {
        "id": "oZjJ_548qVAX"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "WNztJkYRiCuu",
        "outputId": "0f51f635-db0f-44bd-da38-9ba07e6f22dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1:\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "LOSS train 1.5908841967582703 test 2.9817464351654053\n",
            "EPOCH 2:\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "LOSS train 0.9996861100196839 test 1.0965909957885742\n",
            "EPOCH 3:\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "LOSS train 0.6261658608913422 test 2.1782004833221436\n",
            "EPOCH 4:\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "LOSS train 0.5674642853438854 test 0.5596820712089539\n",
            "EPOCH 5:\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "LOSS train 0.70390006005764 test 0.1718156486749649\n",
            "EPOCH 6:\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "LOSS train 0.6155088005587459 test 0.10995296388864517\n",
            "EPOCH 7:\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "LOSS train 0.5728276938199997 test 0.20453256368637085\n",
            "EPOCH 8:\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "LOSS train 0.4085582785308361 test 0.15765275061130524\n",
            "EPOCH 9:\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "LOSS train 0.25984447821974754 test 0.17959147691726685\n",
            "EPOCH 10:\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "input is  torch.Size([3, 224, 224])\n",
            "torch.Size([5])\n",
            "LOSS train 0.28837043726816775 test 0.2447887808084488\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "epoch_number = 0\n",
        "best_vloss = 1_000_000.\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    model.train(True)\n",
        "    avg_loss = train_one_epoch(epoch_number)\n",
        "\n",
        "    # We don't need gradients on to do reporting\n",
        "    model.train(False)\n",
        "\n",
        "    running_test_loss = 0.0\n",
        "    for i, test_data in enumerate(test_loader):\n",
        "        test_inputs, test_labels = test_data\n",
        "        test_outputs = model(test_inputs)\n",
        "        test_loss = loss_fn(test_outputs, test_labels)\n",
        "        running_test_loss += test_loss\n",
        "\n",
        "    avg_test_loss = running_test_loss / (i + 1)\n",
        "    print('LOSS train {} test {}'.format(avg_loss, avg_test_loss))\n",
        "\n",
        "    # Log the running loss averaged per batch\n",
        "    # for both training and validation\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    # if avg_test_loss < best_vloss:\n",
        "    #     best_vloss = avg_test_loss\n",
        "    #     model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
        "    #     torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    epoch_number += 1\n",
        "    if epoch_number == 5 or epoch_number == 10:\n",
        "      path = \"efficient_net_models/model_{}epochs\".format(epoch_number)\n",
        "      torch.save(model, path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YdzfiD0fuKLC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}